{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "import mercury as mr\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, JSONLoader\n",
    "import pandas as pd\n",
    "import json\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*Python version.*google.api_core.*\")\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pickle\n",
    "\n",
    "from googleapiclient.errors import HttpError\n",
    "import base64\n",
    "from email import message_from_bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63c513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory_data = 'C:/Users/maran/OneDrive/Documents/Git Profile/Data-Projects/Inference Group/rag_data/'\n",
    "state_of_union_file = '2024_State_of_the_Union.txt'\n",
    "rag_data_dir = '/rag_data'\n",
    "vector_store_faiss = '/faiss_index'\n",
    "path_to_faiss_dir = local_directory_data + rag_data_dir + vector_store_faiss\n",
    "path_to_state_of_union = local_directory_data + state_of_union_file\n",
    "\n",
    "# Custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "opening_question = \"What do you want to know?\"\n",
    "chat_messages = []\n",
    "chat_messages.append(opening_question)\n",
    "\n",
    "web_crawl_dir = '/web_crawled_data'\n",
    "path_to_web_crawl_dir = local_directory_data + rag_data_dir + web_crawl_dir\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path_to_web_crawl_dir, exist_ok=True)\n",
    "\n",
    "# Add these variables after your existing variable definitions\n",
    "calendar_data_dir = '/calendar_data'\n",
    "path_to_calendar_dir = local_directory_data + rag_data_dir + calendar_data_dir\n",
    "os.makedirs(path_to_calendar_dir, exist_ok=True)\n",
    "\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/calendar.readonly',\n",
    "    'https://www.googleapis.com/auth/gmail.readonly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de93bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.App(\n",
    "    title=\"Simple RAG Query System\", \n",
    "    description=\"Ask questions about your documents\",\n",
    "    show_code=False,\n",
    "    static_notebook=False \n",
    ")\n",
    "\n",
    "question = mr.Text(value=None, label=opening_question)\n",
    "\n",
    "search_result_choices = mr.Slider(value=1, min=1, max=5, label=\"Number of results\")\n",
    "show_sources = mr.Checkbox(value=False, label=\"Show Sources\")\n",
    "file_upload = mr.File(label=\"File upload\", max_file_size=\"10MB\")\n",
    "\n",
    "url_input = mr.Text(value=None, label=\"Enter website URL to crawl\")\n",
    "crawl_depth = mr.Slider(value=1, min=1, max=3, label=\"Crawl depth\")\n",
    "max_pages = mr.Slider(value=10, min=1, max=50, label=\"Max pages to crawl\")\n",
    "\n",
    "calendar_connect = mr.Checkbox(value=False, label=\"Connect to Google Calendar\")\n",
    "event_direction = mr.Select(value=\"future\", choices=[\"future\", \"past\", \"both\"], label=\"Event Direction\")\n",
    "days_to_fetch = mr.Slider(value=7, min=1, max=360, label=\"Days of events to fetch (future)\")\n",
    "days_to_look_back = mr.Slider(value=7, min=1, max=360, label=\"Days to look back (past)\")\n",
    "calendar_credentials = mr.File(label=\"Upload credentials.json\", max_file_size=\"1MB\")\n",
    "calendar_status = mr.Text(value=\"Calendar not connected\", label=\"Status\")\n",
    "\n",
    "gmail_connect = mr.Checkbox(value=False, label=\"Connect to Gmail\")\n",
    "gmail_credentials = mr.File(label=\"Upload Gmail credentials.json\", max_file_size=\"1MB\")\n",
    "max_emails = mr.Slider(value=10, min=1, max=50, label=\"Max promotion emails to fetch\")\n",
    "gmail_status = mr.Text(value=\"Gmail not connected\", label=\"Gmail Status\")\n",
    "\n",
    "news_fetch = mr.Checkbox(value=False, label=\"Fetch News Articles\")\n",
    "max_news_articles = mr.Slider(value=10, min=1, max=50, label=\"Max news articles to fetch\")\n",
    "news_category = mr.Select(value=\"general\", choices=[\"general\", \"business\", \"entertainment\", \"health\", \"science\", \"sports\", \"technology\"], label=\"News Category\")\n",
    "news_status = mr.Text(value=\"News not fetched\", label=\"News Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa48c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell for web crawling functionality\n",
    "def crawl_website(start_url, max_depth=1, max_pages=10):\n",
    "    \"\"\"\n",
    "    Crawl a website and extract text content from pages\n",
    "    \"\"\"\n",
    "    visited_urls = set()\n",
    "    pages_content = []\n",
    "    \n",
    "    def extract_page_content(url, depth):\n",
    "        if (len(visited_urls) >= max_pages or \n",
    "            url in visited_urls or \n",
    "            depth > max_depth):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            print(f\"Crawling: {url} (depth {depth})\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up text\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            if text:\n",
    "                pages_content.append({\n",
    "                    'url': url,\n",
    "                    'content': text,\n",
    "                    'depth': depth\n",
    "                })\n",
    "            \n",
    "            visited_urls.add(url)\n",
    "            \n",
    "            # Extract and follow links if within depth limit\n",
    "            if depth < max_depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    next_url = urljoin(url, link['href'])\n",
    "                    \n",
    "                    # Basic URL filtering\n",
    "                    parsed_url = urlparse(next_url)\n",
    "                    parsed_start = urlparse(start_url)\n",
    "                    \n",
    "                    # Only follow links from same domain\n",
    "                    if (parsed_url.netloc == parsed_start.netloc and\n",
    "                        next_url not in visited_urls and\n",
    "                        len(visited_urls) < max_pages):\n",
    "                        \n",
    "                        extract_page_content(next_url, depth + 1)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {str(e)}\")\n",
    "    \n",
    "    extract_page_content(start_url, 0)\n",
    "    return pages_content\n",
    "\n",
    "def save_crawled_data(pages_content, filename_prefix=\"crawled\"):\n",
    "    \"\"\"\n",
    "    Save crawled content to files\n",
    "    \"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.txt\"\n",
    "    filepath = os.path.join(path_to_web_crawl_dir, filename)\n",
    "    \n",
    "    all_content = \"\"\n",
    "    for i, page in enumerate(pages_content):\n",
    "        all_content += f\"--- Page {i+1}: {page['url']} (Depth {page['depth']}) ---\\\\n\"\n",
    "        all_content += page['content'] + \"\\\\n\\\\n\"\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(all_content)\n",
    "    \n",
    "    return filepath, all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74482677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API is working!'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    # print(\"‚úÖ API Response:\", response.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå API Error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b228873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_API_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "\n",
    "try:\n",
    "    url = f\"https://gnews.io/api/v4/top-headlines?category=general&lang=en&apikey={NEWS_API_KEY}\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        mr.Note(\"‚úÖ API is working!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è API returned status code: {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API test failed: {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20deda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_upload.filepath is not None:\n",
    "    file_extension = os.path.splitext(file_upload.filepath)[1].lower()\n",
    "    file_content = \"\"\n",
    "    \n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            df = pd.read_csv(file_upload.filepath)\n",
    "            file_content = df.to_string(index=False)\n",
    "            \n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_upload.filepath)\n",
    "            file_content = df.to_string(index=False)\n",
    "            \n",
    "        elif file_extension == '.pdf':\n",
    "            with open(file_upload.filepath, 'rb') as pdf_file:\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    file_content += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "        elif file_extension == '.json':\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                file_content = json.dumps(json_data, indent=2)\n",
    "                \n",
    "        elif file_extension == '.txt':\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as text_file:\n",
    "                file_content = text_file.read()\n",
    "                \n",
    "        else:\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as file_obj:\n",
    "                file_content = file_obj.read()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        \n",
    "        example_texts = text_splitter.split_text(file_content)\n",
    "        documents = [Document(page_content=text) for text in example_texts]\n",
    "        \n",
    "        # Create embeddings and vector store\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        vector_store.save_local(path_to_faiss_dir)\n",
    "        \n",
    "        retriever_object = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": search_result_choices.value} \n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to handle web crawling in your main flow\n",
    "crawled_content = \"\"\n",
    "crawled_file_path = \"\"\n",
    "\n",
    "if url_input.value and url_input.value.strip():\n",
    "    try:\n",
    "        pages_content = crawl_website(\n",
    "            start_url=url_input.value.strip(),\n",
    "            max_depth=crawl_depth.value,\n",
    "            max_pages=max_pages.value\n",
    "        )\n",
    "        \n",
    "        if pages_content:\n",
    "            crawled_file_path, crawled_content = save_crawled_data(pages_content)\n",
    "            # mr.Output(f\"‚úÖ Successfully crawled {len(pages_content)} pages from {url_input.value}\")\n",
    "            \n",
    "            # Process the crawled content for RAG\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=100\n",
    "            )\n",
    "            \n",
    "            crawled_texts = text_splitter.split_text(crawled_content)\n",
    "            crawled_documents = [Document(page_content=text) for text in crawled_texts]\n",
    "            \n",
    "            # Create embeddings and vector store for crawled content\n",
    "            embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "            vector_store = FAISS.from_documents(crawled_documents, embeddings)\n",
    "            vector_store.save_local(path_to_faiss_dir)\n",
    "            \n",
    "            retriever_object = vector_store.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": search_result_choices.value}\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error crawling website: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ce4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell after your web crawling cell (around line 40)\n",
    "calendar_content = \"\"\n",
    "calendar_connected = False\n",
    "\n",
    "if calendar_connect.value and calendar_credentials.filepath is not None:\n",
    "    creds_path = os.path.join(path_to_calendar_dir, 'credentials.json')\n",
    "    with open(calendar_credentials.filepath, 'rb') as src_file:\n",
    "        with open(creds_path, 'wb') as dst_file:\n",
    "            dst_file.write(src_file.read())\n",
    "\n",
    "    creds = None\n",
    "    token_path = os.path.join(path_to_calendar_dir, 'token.pickle')\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        with open(token_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(creds_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        with open(token_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('calendar', 'v3', credentials=creds)\n",
    "    now = datetime.now(timezone.utc)\n",
    "    now_iso = now.isoformat().replace('+00:00', 'Z')\n",
    "    \n",
    "    all_events = []\n",
    "    calendar_content = \"\"\n",
    "    \n",
    "    # Fetch future events if requested\n",
    "    if event_direction.value in [\"future\", \"both\"]:\n",
    "        end_date = (now + timedelta(days=days_to_fetch.value)).isoformat().replace('+00:00', 'Z')\n",
    "        \n",
    "        future_events_result = service.events().list(\n",
    "            calendarId='primary',\n",
    "            timeMin=now_iso,\n",
    "            timeMax=end_date,\n",
    "            maxResults=50,\n",
    "            singleEvents=True,\n",
    "            orderBy='startTime'\n",
    "        ).execute()\n",
    "        \n",
    "        future_events = future_events_result.get('items', [])\n",
    "        for event in future_events:\n",
    "            event['_direction'] = 'future'\n",
    "            all_events.append(event)\n",
    "    \n",
    "    # Fetch past events if requested\n",
    "    if event_direction.value in [\"past\", \"both\"]:\n",
    "        start_date = (now - timedelta(days=days_to_look_back.value)).isoformat().replace('+00:00', 'Z')\n",
    "        \n",
    "        past_events_result = service.events().list(\n",
    "            calendarId='primary',\n",
    "            timeMin=start_date,\n",
    "            timeMax=now_iso,\n",
    "            maxResults=50,\n",
    "            singleEvents=True,\n",
    "            orderBy='startTime'\n",
    "        ).execute()\n",
    "        \n",
    "        past_events = past_events_result.get('items', [])\n",
    "        for event in past_events:\n",
    "            event['_direction'] = 'past'\n",
    "            all_events.append(event)\n",
    "    \n",
    "    # Sort all events by start time\n",
    "    def get_event_start_time(event):\n",
    "        start = event['start'].get('dateTime', event['start'].get('date'))\n",
    "        try:\n",
    "            # Try parsing with timezone\n",
    "            if 'T' in start:\n",
    "                # Has time component - parse with timezone\n",
    "                dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n",
    "                # Ensure it's timezone-aware\n",
    "                if dt.tzinfo is None:\n",
    "                    dt = dt.replace(tzinfo=timezone.utc)\n",
    "                return dt\n",
    "            else:\n",
    "                # Date-only event - treat as UTC midnight\n",
    "                dt = datetime.fromisoformat(start)\n",
    "                return dt.replace(tzinfo=timezone.utc)\n",
    "        except:\n",
    "            # Fallback to current time if parsing fails\n",
    "            return now\n",
    "    \n",
    "    all_events.sort(key=get_event_start_time, reverse=(event_direction.value == \"past\"))\n",
    "    \n",
    "    if not all_events:\n",
    "        if event_direction.value == \"future\":\n",
    "            calendar_content = f\"No upcoming events found in the next {days_to_fetch.value} days.\"\n",
    "        elif event_direction.value == \"past\":\n",
    "            calendar_content = f\"No past events found in the last {days_to_look_back.value} days.\"\n",
    "        else:\n",
    "            calendar_content = f\"No events found (past {days_to_look_back.value} days or next {days_to_fetch.value} days).\"\n",
    "    else:\n",
    "        # Build content header\n",
    "        if event_direction.value == \"future\":\n",
    "            calendar_content = f\"UPCOMING CALENDAR EVENTS (Next {days_to_fetch.value} days):\\n\\n\"\n",
    "        elif event_direction.value == \"past\":\n",
    "            calendar_content = f\"PAST CALENDAR EVENTS (Last {days_to_look_back.value} days):\\n\\n\"\n",
    "        else:\n",
    "            calendar_content = f\"CALENDAR EVENTS (Past {days_to_look_back.value} days & Next {days_to_fetch.value} days):\\n\\n\"\n",
    "        \n",
    "        for event in all_events:\n",
    "            start = event['start'].get('dateTime', event['start'].get('date'))\n",
    "            end = event['end'].get('dateTime', event['end'].get('date'))\n",
    "            summary = event.get('summary', 'No title')\n",
    "            description = event.get('description', 'No description')\n",
    "            direction = event.get('_direction', 'unknown')\n",
    "            \n",
    "            direction_label = \"üîÆ FUTURE\" if direction == 'future' else \"üìú PAST\"\n",
    "            \n",
    "            calendar_content += f\"{direction_label} - Event: {summary}\\n\"\n",
    "            calendar_content += f\"Time: {start} to {end}\\n\"\n",
    "            calendar_content += f\"Description: {description}\\n\"\n",
    "            calendar_content += \"‚îÄ\" * 50 + \"\\n\"\n",
    "    \n",
    "    calendar_connected = True\n",
    "    future_count = sum(1 for e in all_events if e.get('_direction') == 'future')\n",
    "    past_count = sum(1 for e in all_events if e.get('_direction') == 'past')\n",
    "    \n",
    "    if event_direction.value == \"both\":\n",
    "        event_load_message = f\"‚úÖ Connected - {len(all_events)} events loaded ({past_count} past, {future_count} future)\"\n",
    "    elif event_direction.value == \"past\":\n",
    "        event_load_message = f\"‚úÖ Connected - {len(all_events)} past events loaded\"\n",
    "    else:\n",
    "        event_load_message = f\"‚úÖ Connected - {len(all_events)} future events loaded\"\n",
    "    \n",
    "    chat_messages.append(event_load_message)\n",
    "    mr.Note(event_load_message)\n",
    "    \n",
    "elif calendar_connect.value and calendar_credentials.filepath is None:\n",
    "    mr.Note(\"‚ùå Please upload credentials.json file\")\n",
    "    calendar_connected = False\n",
    "else:\n",
    "    calendar_connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News fetching functionality (GNews API only)\n",
    "news_content = \"\"\n",
    "news_connected = False\n",
    "\n",
    "if news_fetch.value:\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        NEWS_API_KEY = os.getenv(\"GNEWS_API_KEY\") or os.getenv(\"NEWSAPI_KEY\")\n",
    "        \n",
    "        if not NEWS_API_KEY:\n",
    "            mr.Note(\"‚ùå Please set GNEWS_API_KEY or NEWSAPI_KEY in your .env file\")\n",
    "            news_connected = False\n",
    "        else:\n",
    "            url = f\"https://gnews.io/api/v4/top-headlines?category={news_category.value}&lang=en&apikey={NEWS_API_KEY}&max={max_news_articles.value}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            # Check for specific HTTP errors\n",
    "            if response.status_code == 401:\n",
    "                error_msg = \"‚ùå GNews API Error: Unauthorized (401). Your API key may be invalid, expired, or you may have exceeded your quota. Please check your GNEWS_API_KEY in the .env file.\"\n",
    "                mr.Note(error_msg)\n",
    "                chat_messages.append(error_msg)\n",
    "                news_connected = False\n",
    "            elif response.status_code == 403:\n",
    "                error_msg = \"‚ùå GNews API Error: Forbidden (403). Your API key may not have permission to access this endpoint.\"\n",
    "                mr.Note(error_msg)\n",
    "                chat_messages.append(error_msg)\n",
    "                news_connected = False\n",
    "            elif not response.ok:\n",
    "                error_msg = f\"‚ùå GNews API Error: HTTP {response.status_code} - {response.text[:200]}\"\n",
    "                mr.Note(error_msg)\n",
    "                chat_messages.append(error_msg)\n",
    "                news_connected = False\n",
    "            else:\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                articles = data.get('articles', [])\n",
    "                \n",
    "                if not articles:\n",
    "                    news_content = \"No news articles found.\"\n",
    "                else:\n",
    "                    news_content = f\"NEWS ARTICLES ({len(articles)} articles from {news_category.value} category):\\n\\n\"\n",
    "                    for i, article in enumerate(articles):\n",
    "                        title = article.get('title', 'No title')\n",
    "                        description = article.get('description', 'No description')\n",
    "                        source = article.get('source', {}).get('name', 'Unknown source')\n",
    "                        url_link = article.get('url', 'No URL')\n",
    "                        published_at = article.get('publishedAt', 'Unknown date')\n",
    "                        \n",
    "                        news_content += f\"Article {i+1}: {title}\\n\"\n",
    "                        news_content += f\"Source: {source}\\n\"\n",
    "                        news_content += f\"Published: {published_at}\\n\"\n",
    "                        news_content += f\"Description: {description}\\n\"\n",
    "                        news_content += f\"URL: {url_link}\\n\"\n",
    "                        news_content += \"‚îÄ\" * 50 + \"\\n\"\n",
    "                \n",
    "                news_connected = True\n",
    "                news_status_msg = f\"‚úÖ Fetched {len(articles)} news articles from GNews\"\n",
    "                chat_messages.append(news_status_msg)\n",
    "                mr.Note(news_status_msg)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # This catches network errors, timeouts, etc. (not HTTP status errors we handle above)\n",
    "        error_msg = f\"‚ùå News API network error: {str(e)}\"\n",
    "        mr.Note(error_msg)\n",
    "        chat_messages.append(error_msg)\n",
    "        news_connected = False\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error fetching news: {str(e)}\"\n",
    "        mr.Note(error_msg)\n",
    "        news_connected = False\n",
    "else:\n",
    "    news_connected = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gmail functionality\n",
    "gmail_content = \"\"\n",
    "gmail_connected = False\n",
    "\n",
    "if gmail_connect.value and gmail_credentials.filepath is not None:\n",
    "    gmail_creds_path = os.path.join(path_to_calendar_dir, 'gmail_credentials.json')\n",
    "    \n",
    "    # Save uploaded credentials\n",
    "    with open(gmail_credentials.filepath, 'rb') as src_file:\n",
    "        with open(gmail_creds_path, 'wb') as dst_file:\n",
    "            dst_file.write(src_file.read())\n",
    "\n",
    "    gmail_creds = None\n",
    "    gmail_token_path = os.path.join(path_to_calendar_dir, 'gmail_token.pickle')\n",
    "    \n",
    "    # Load existing token if available\n",
    "    if os.path.exists(gmail_token_path):\n",
    "        with open(gmail_token_path, 'rb') as token:\n",
    "            gmail_creds = pickle.load(token)\n",
    "    \n",
    "    # Get new credentials if needed\n",
    "    if not gmail_creds or not gmail_creds.valid:\n",
    "        if gmail_creds and gmail_creds.expired and gmail_creds.refresh_token:\n",
    "            gmail_creds.refresh(Request())\n",
    "        else:\n",
    "            gmail_flow = InstalledAppFlow.from_client_secrets_file(gmail_creds_path, SCOPES)\n",
    "            gmail_creds = gmail_flow.run_local_server(port=0)\n",
    "        \n",
    "        # Save the credentials for next run\n",
    "        with open(gmail_token_path, 'wb') as token:\n",
    "            pickle.dump(gmail_creds, token)\n",
    "\n",
    "    # Build Gmail service\n",
    "    gmail_service = build('gmail', 'v1', credentials=gmail_creds)\n",
    "    \n",
    "    try:\n",
    "        # Search for emails in Promotions category\n",
    "        promotions_query = \"category:promotions\"\n",
    "        results = gmail_service.users().messages().list(\n",
    "            userId='me', \n",
    "            q=promotions_query, \n",
    "            maxResults=max_emails.value\n",
    "        ).execute()\n",
    "        \n",
    "        messages = results.get('messages', [])\n",
    "        gmail_content = f\"PROMOTION EMAILS ({len(messages)} emails):\\\\n\\\\n\"\n",
    "        \n",
    "        if not messages:\n",
    "            gmail_content = \"No promotion emails found in your Gmail.\"\n",
    "        else:\n",
    "            for i, message in enumerate(messages):\n",
    "                msg = gmail_service.users().messages().get(\n",
    "                    userId='me', \n",
    "                    id=message['id'], \n",
    "                    format='raw'\n",
    "                ).execute()\n",
    "                \n",
    "                # Decode the message\n",
    "                msg_str = base64.urlsafe_b64decode(msg['raw'].encode('ASCII'))\n",
    "                mime_msg = message_from_bytes(msg_str)\n",
    "                \n",
    "                # Extract subject\n",
    "                subject = \"\"\n",
    "                for part in mime_msg.walk():\n",
    "                    if part.get_content_type() == \"text/plain\":\n",
    "                        subject = part.get(\"Subject\", \"No Subject\")\n",
    "                        break\n",
    "                \n",
    "                if not subject:\n",
    "                    subject = \"No Subject\"\n",
    "                \n",
    "                # Extract snippet\n",
    "                snippet = msg.get('snippet', 'No preview available')\n",
    "                \n",
    "                gmail_content += f\"Email {i+1}: {subject}\\\\n\"\n",
    "                gmail_content += f\"Preview: {snippet}\\\\n\"\n",
    "                gmail_content += \"‚îÄ\" * 50 + \"\\\\n\"\n",
    "        \n",
    "        gmail_connected = True\n",
    "        good_gmail_status = f\"‚úÖ Connected - {len(messages)} promotion emails loaded\"\n",
    "        chat_messages.append(good_gmail_status)\n",
    "        mr.Note(good_gmail_status)\n",
    "        \n",
    "    except HttpError as error:\n",
    "        bad_gmail_status = f\"‚ùå Gmail error: {error}\"\n",
    "        mr.Note(bad_gmail_status)\n",
    "        \n",
    "elif gmail_connect.value and gmail_credentials.filepath is None:\n",
    "    mr.Note(\"‚ùå Please upload Gmail credentials.json file\")\n",
    "    gmail_connected = False\n",
    "else:\n",
    "    gmail_connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95010d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Chain setup with all data sources\n",
    "llm_object = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Determine which retriever to use (priority: file > gmail > calendar > web > none)\n",
    "current_retriever = None\n",
    "context_source = \"general knowledge\"\n",
    "\n",
    "if file_upload.filepath is not None:\n",
    "    current_retriever = retriever_object\n",
    "    context_source = \"uploaded file\"\n",
    "    \n",
    "elif gmail_connected and gmail_content:\n",
    "    # Process Gmail data for RAG\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    gmail_texts = text_splitter.split_text(gmail_content)\n",
    "    gmail_documents = [Document(page_content=text) for text in gmail_texts]\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(gmail_documents, embeddings)\n",
    "    vector_store.save_local(path_to_faiss_dir)\n",
    "    \n",
    "    current_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": search_result_choices.value}\n",
    "    )\n",
    "    context_source = \"Gmail promotion emails\"\n",
    "    \n",
    "elif calendar_connected and calendar_content:\n",
    "    # Process calendar data for RAG\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    calendar_texts = text_splitter.split_text(calendar_content)\n",
    "    calendar_documents = [Document(page_content=text) for text in calendar_texts]\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(calendar_documents, embeddings)\n",
    "    vector_store.save_local(path_to_faiss_dir)\n",
    "    \n",
    "    current_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": search_result_choices.value}\n",
    "    )\n",
    "    context_source = \"calendar data\"\n",
    "    \n",
    "elif url_input.value and url_input.value.strip() and crawled_content:\n",
    "    current_retriever = retriever_object\n",
    "    context_source = \"web data\"\n",
    "    \n",
    "else:\n",
    "    current_retriever = None\n",
    "    context_source = \"general knowledge\"\n",
    "\n",
    "# Create the appropriate QA chain\n",
    "if current_retriever is None:\n",
    "    normal_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Answer the following question: {question}\"\n",
    "    )\n",
    "    qa_chain = normal_prompt | llm_object\n",
    "else:\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, \n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm_object,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=current_retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=show_sources.value\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b06584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your final chat display cell to include calendar context\n",
    "if url_input.value and url_input.value.strip() and crawled_content:\n",
    "    chat_messages.append(f\"üåê Using content crawled from: {url_input.value}\")\n",
    "    chat_messages.append(f\"üìÑ Crawled {len(crawled_content.split('--- Page'))} pages\")\n",
    "\n",
    "if calendar_connected and calendar_content:\n",
    "    chat_messages.append(f\"üìÖ Using calendar data with {len(calendar_content.split('Event:'))-1} events\")\n",
    "\n",
    "if gmail_connected and gmail_content:\n",
    "    chat_messages.append(f\"üìß Using Gmail promotions data with {len(gmail_content.split('Email'))-1} emails\")\n",
    "\n",
    "if news_connected and news_content:\n",
    "    chat_messages.append(f\"üì∞ Using news data with {len(news_content.split('Article'))-1} articles\")\n",
    "\n",
    "if current_retriever is None:\n",
    "    normal_no_context_result = qa_chain.invoke({\"question\": question.value})\n",
    "    llm_result = {'result': normal_no_context_result.content}\n",
    "else:\n",
    "    llm_result = qa_chain.invoke({\"query\": question.value})\n",
    "\n",
    "if question.value != '':\n",
    "    if show_sources.value:\n",
    "        sources_result_string = f\"\\nüìö SOURCES ({len(llm_result.get('source_documents', []))} documents):\"\n",
    "        for i, doc in enumerate(llm_result.get('source_documents', [])):\n",
    "            sources_result_string += f\"\\n\\nSource {i+1}:\"\n",
    "            sources_result_string += f\"\\nContent: {doc.page_content}...\"\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                sources_result_string += f\"\\nMetadata: {doc.metadata}\"\n",
    "    else:\n",
    "        sources_result_string = ''\n",
    "\n",
    "    chat_messages.append(question.value)\n",
    "    chat_messages.append(llm_result['result'] + sources_result_string)\n",
    "\n",
    "mr.Chat(chat_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b38d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
