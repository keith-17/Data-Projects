{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "import mercury as mr\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df2f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory_data = 'C:/Users/maran/OneDrive/Documents/Git Profile/Data-Projects/Inference Group/rag_data/'\n",
    "state_of_union_file = '2024_State_of_the_Union.txt'\n",
    "rag_data_dir = '/rag_data'\n",
    "vector_store_faiss = '/faiss_index'\n",
    "path_to_faiss_dir = local_directory_data + rag_data_dir + vector_store_faiss\n",
    "path_to_state_of_union = local_directory_data + state_of_union_file\n",
    "\n",
    "# Custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de93bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.App(\n",
    "    title=\"Simple RAG Query System\", \n",
    "    description=\"Ask questions about your documents\",\n",
    "    show_code=False,\n",
    "    static_notebook=False \n",
    ")\n",
    "\n",
    "question = mr.Text(value=None, label=\"What do you want to know?\")\n",
    "search_result_choices = mr.Slider(value=3, min=1, max=5, label=\"Number of results\")\n",
    "show_sources = mr.Checkbox(value=False, label=\"Show Sources\")\n",
    "file_upload = mr.File(label=\"File upload\", max_file_size=\"10MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf7307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API is working!'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    print(\"‚úÖ API Response:\", response.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå API Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102978b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "if file_upload.filepath is not None:\n",
    "    with open(file_upload.filepath,  'r', encoding='utf-8') as file_upload_obj:\n",
    "        file_upload_read_obj = file_upload_obj.read()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    example_texts = text_splitter.split_text(file_upload_read_obj)\n",
    "    documents = [Document(page_content=text) for text in example_texts]\n",
    "\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    vector_store.save_local(path_to_faiss_dir)\n",
    "\n",
    "    retriever_object = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": search_result_choices.value} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24068597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "if file_upload.filepath is None:\n",
    "    normal_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Answer the following question: {question}\"\n",
    "    )\n",
    "    qa_chain = normal_prompt | llm\n",
    "\n",
    "else:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever_object,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=show_sources.value\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93928a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_upload.filepath is None:\n",
    "    normal_no_context_result: dict = qa_chain.invoke({\"question\": question.value})\n",
    "    llm_result = {'result': normal_no_context_result.content}\n",
    "else:\n",
    "    llm_result: dict = qa_chain.invoke({\"query\": question.value})\n",
    "\n",
    "if question.value != '':\n",
    "    print(llm_result['result'])\n",
    "\n",
    "if show_sources.value:\n",
    "    print(f\"\\nüìö SOURCES ({len(llm_result.get('source_documents', []))} documents):\")\n",
    "    for i, doc in enumerate(llm_result.get('source_documents', [])):\n",
    "        print(f\"\\nSource {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content}...\")\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "            print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1f800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
