{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "import mercury as mr\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, JSONLoader\n",
    "import pandas as pd\n",
    "import json\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*Python version.*google.api_core.*\")\n",
    "\n",
    "# Add these imports after your existing imports\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63c513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory_data = 'C:/Users/maran/OneDrive/Documents/Git Profile/Data-Projects/Inference Group/rag_data/'\n",
    "state_of_union_file = '2024_State_of_the_Union.txt'\n",
    "rag_data_dir = '/rag_data'\n",
    "vector_store_faiss = '/faiss_index'\n",
    "path_to_faiss_dir = local_directory_data + rag_data_dir + vector_store_faiss\n",
    "path_to_state_of_union = local_directory_data + state_of_union_file\n",
    "\n",
    "# Custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "opening_question = \"What do you want to know?\"\n",
    "chat_messages = []\n",
    "chat_messages.append(opening_question)\n",
    "\n",
    "web_crawl_dir = '/web_crawled_data'\n",
    "path_to_web_crawl_dir = local_directory_data + rag_data_dir + web_crawl_dir\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path_to_web_crawl_dir, exist_ok=True)\n",
    "\n",
    "# Add these variables after your existing variable definitions\n",
    "calendar_data_dir = '/calendar_data'\n",
    "path_to_calendar_dir = local_directory_data + rag_data_dir + calendar_data_dir\n",
    "os.makedirs(path_to_calendar_dir, exist_ok=True)\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/calendar.readonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de93bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = mr.App(\n",
    "    title=\"Simple RAG Query System\", \n",
    "    description=\"Ask questions about your documents\",\n",
    "    show_code=False,\n",
    "    static_notebook=False \n",
    ")\n",
    "\n",
    "question = mr.Text(value=None, label=opening_question)\n",
    "\n",
    "search_result_choices = mr.Slider(value=1, min=1, max=5, label=\"Number of results\")\n",
    "show_sources = mr.Checkbox(value=False, label=\"Show Sources\")\n",
    "file_upload = mr.File(label=\"File upload\", max_file_size=\"10MB\")\n",
    "\n",
    "url_input = mr.Text(value=None, label=\"Enter website URL to crawl\")\n",
    "crawl_depth = mr.Slider(value=1, min=1, max=3, label=\"Crawl depth\")\n",
    "max_pages = mr.Slider(value=10, min=1, max=50, label=\"Max pages to crawl\")\n",
    "\n",
    "calendar_connect = mr.Checkbox(value=False, label=\"Connect to Google Calendar\")\n",
    "days_to_fetch = mr.Slider(value=7, min=1, max=30, label=\"Days of events to fetch\")\n",
    "calendar_credentials = mr.File(label=\"Upload credentials.json\", max_file_size=\"1MB\")\n",
    "calendar_status = mr.Text(value=\"Calendar not connected\", label=\"Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa48c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell for web crawling functionality\n",
    "def crawl_website(start_url, max_depth=1, max_pages=10):\n",
    "    \"\"\"\n",
    "    Crawl a website and extract text content from pages\n",
    "    \"\"\"\n",
    "    visited_urls = set()\n",
    "    pages_content = []\n",
    "    \n",
    "    def extract_page_content(url, depth):\n",
    "        if (len(visited_urls) >= max_pages or \n",
    "            url in visited_urls or \n",
    "            depth > max_depth):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            print(f\"Crawling: {url} (depth {depth})\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text content\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up text\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            if text:\n",
    "                pages_content.append({\n",
    "                    'url': url,\n",
    "                    'content': text,\n",
    "                    'depth': depth\n",
    "                })\n",
    "            \n",
    "            visited_urls.add(url)\n",
    "            \n",
    "            # Extract and follow links if within depth limit\n",
    "            if depth < max_depth:\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    next_url = urljoin(url, link['href'])\n",
    "                    \n",
    "                    # Basic URL filtering\n",
    "                    parsed_url = urlparse(next_url)\n",
    "                    parsed_start = urlparse(start_url)\n",
    "                    \n",
    "                    # Only follow links from same domain\n",
    "                    if (parsed_url.netloc == parsed_start.netloc and\n",
    "                        next_url not in visited_urls and\n",
    "                        len(visited_urls) < max_pages):\n",
    "                        \n",
    "                        extract_page_content(next_url, depth + 1)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {str(e)}\")\n",
    "    \n",
    "    extract_page_content(start_url, 0)\n",
    "    return pages_content\n",
    "\n",
    "def save_crawled_data(pages_content, filename_prefix=\"crawled\"):\n",
    "    \"\"\"\n",
    "    Save crawled content to files\n",
    "    \"\"\"\n",
    "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.txt\"\n",
    "    filepath = os.path.join(path_to_web_crawl_dir, filename)\n",
    "    \n",
    "    all_content = \"\"\n",
    "    for i, page in enumerate(pages_content):\n",
    "        all_content += f\"--- Page {i+1}: {page['url']} (Depth {page['depth']}) ---\\\\n\"\n",
    "        all_content += page['content'] + \"\\\\n\\\\n\"\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(all_content)\n",
    "    \n",
    "    return filepath, all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74482677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3502ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say 'API is working!'\"}],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    # print(\"‚úÖ API Response:\", response.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå API Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e3de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_upload.filepath is not None:\n",
    "    file_extension = os.path.splitext(file_upload.filepath)[1].lower()\n",
    "    file_content = \"\"\n",
    "    \n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            df = pd.read_csv(file_upload.filepath)\n",
    "            file_content = df.to_string(index=False)\n",
    "            \n",
    "        elif file_extension in ['.xlsx', '.xls']:\n",
    "            df = pd.read_excel(file_upload.filepath)\n",
    "            file_content = df.to_string(index=False)\n",
    "            \n",
    "        elif file_extension == '.pdf':\n",
    "            with open(file_upload.filepath, 'rb') as pdf_file:\n",
    "                pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    file_content += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "        elif file_extension == '.json':\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                file_content = json.dumps(json_data, indent=2)\n",
    "                \n",
    "        elif file_extension == '.txt':\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as text_file:\n",
    "                file_content = text_file.read()\n",
    "                \n",
    "        else:\n",
    "            with open(file_upload.filepath, 'r', encoding='utf-8') as file_obj:\n",
    "                file_content = file_obj.read()\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        \n",
    "        example_texts = text_splitter.split_text(file_content)\n",
    "        documents = [Document(page_content=text) for text in example_texts]\n",
    "        \n",
    "        # Create embeddings and vector store\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        \n",
    "        vector_store = FAISS.from_documents(documents, embeddings)\n",
    "        vector_store.save_local(path_to_faiss_dir)\n",
    "        \n",
    "        retriever_object = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": search_result_choices.value} \n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to handle web crawling in your main flow\n",
    "crawled_content = \"\"\n",
    "crawled_file_path = \"\"\n",
    "\n",
    "if url_input.value and url_input.value.strip():\n",
    "    try:\n",
    "        pages_content = crawl_website(\n",
    "            start_url=url_input.value.strip(),\n",
    "            max_depth=crawl_depth.value,\n",
    "            max_pages=max_pages.value\n",
    "        )\n",
    "        \n",
    "        if pages_content:\n",
    "            crawled_file_path, crawled_content = save_crawled_data(pages_content)\n",
    "            # mr.Output(f\"‚úÖ Successfully crawled {len(pages_content)} pages from {url_input.value}\")\n",
    "            \n",
    "            # Process the crawled content for RAG\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1000,\n",
    "                chunk_overlap=100\n",
    "            )\n",
    "            \n",
    "            crawled_texts = text_splitter.split_text(crawled_content)\n",
    "            crawled_documents = [Document(page_content=text) for text in crawled_texts]\n",
    "            \n",
    "            # Create embeddings and vector store for crawled content\n",
    "            embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "            vector_store = FAISS.from_documents(crawled_documents, embeddings)\n",
    "            vector_store.save_local(path_to_faiss_dir)\n",
    "            \n",
    "            retriever_object = vector_store.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": search_result_choices.value}\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error crawling website: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ce4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell after your web crawling cell (around line 40)\n",
    "calendar_content = \"\"\n",
    "calendar_connected = False\n",
    "\n",
    "if calendar_connect.value and calendar_credentials.filepath is not None:\n",
    "    calendar_status.value = \"Authenticating with Google Calendar...\"\n",
    "    \n",
    "    creds_path = os.path.join(path_to_calendar_dir, 'credentials.json')\n",
    "    with open(calendar_credentials.filepath, 'rb') as src_file:\n",
    "        with open(creds_path, 'wb') as dst_file:\n",
    "            dst_file.write(src_file.read())\n",
    "\n",
    "    creds = None\n",
    "    token_path = os.path.join(path_to_calendar_dir, 'token.pickle')\n",
    "    \n",
    "    if os.path.exists(token_path):\n",
    "        with open(token_path, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(creds_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        with open(token_path, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "\n",
    "    service = build('calendar', 'v3', credentials=creds)\n",
    "    now = datetime.utcnow().isoformat() + 'Z'\n",
    "    end_date = (datetime.utcnow() + timedelta(days=days_to_fetch.value)).isoformat() + 'Z'\n",
    "    \n",
    "    events_result = service.events().list(\n",
    "        calendarId='primary',\n",
    "        timeMin=now,\n",
    "        timeMax=end_date,\n",
    "        maxResults=50,\n",
    "        singleEvents=True,\n",
    "        orderBy='startTime'\n",
    "    ).execute()\n",
    "    \n",
    "    events = events_result.get('items', [])\n",
    "    \n",
    "    if not events:\n",
    "        calendar_content = \"No upcoming events found.\"\n",
    "    else:\n",
    "        calendar_content = f\"UPCOMING CALENDAR EVENTS (Next {days_to_fetch.value} days):\\n\\n\"\n",
    "        for event in events:\n",
    "            start = event['start'].get('dateTime', event['start'].get('date'))\n",
    "            end = event['end'].get('dateTime', event['end'].get('date'))\n",
    "            summary = event.get('summary', 'No title')\n",
    "            description = event.get('description', 'No description')\n",
    "            \n",
    "            calendar_content += f\"Event: {summary}\\n\"\n",
    "            calendar_content += f\"Time: {start} to {end}\\n\"\n",
    "            calendar_content += f\"Description: {description}\\n\"\n",
    "            calendar_content += \"‚îÄ\" * 50 + \"\\n\"\n",
    "    \n",
    "    calendar_connected = True\n",
    "    calendar_status.value = f\"‚úÖ Connected - {len(events)} events loaded\"\n",
    "    \n",
    "elif calendar_connect.value and calendar_credentials.filepath is None:\n",
    "    calendar_status.value = \"‚ùå Please upload credentials.json file\"\n",
    "    calendar_connected = False\n",
    "else:\n",
    "    calendar_connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95010d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bdd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Chain setup with all data sources\n",
    "llm_object = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Determine which retriever to use (priority: file > calendar > web > none)\n",
    "current_retriever = None\n",
    "context_source = \"general knowledge\"\n",
    "\n",
    "if file_upload.filepath is not None:\n",
    "    # Process file upload\n",
    "    current_retriever = retriever_object\n",
    "    context_source = \"uploaded file\"\n",
    "    \n",
    "elif calendar_connected and calendar_content:\n",
    "    # Process calendar data for RAG\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    \n",
    "    calendar_texts = text_splitter.split_text(calendar_content)\n",
    "    calendar_documents = [Document(page_content=text) for text in calendar_texts]\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = FAISS.from_documents(calendar_documents, embeddings)\n",
    "    vector_store.save_local(path_to_faiss_dir)\n",
    "    \n",
    "    current_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": search_result_choices.value}\n",
    "    )\n",
    "    context_source = \"calendar data\"\n",
    "    \n",
    "elif url_input.value and url_input.value.strip() and crawled_content:\n",
    "    # Process web data\n",
    "    current_retriever = retriever_object\n",
    "    context_source = \"web data\"\n",
    "    \n",
    "else:\n",
    "    # No context\n",
    "    current_retriever = None\n",
    "    context_source = \"general knowledge\"\n",
    "\n",
    "# Create the appropriate QA chain\n",
    "if current_retriever is None:\n",
    "    normal_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Answer the following question: {question}\"\n",
    "    )\n",
    "    qa_chain = normal_prompt | llm_object\n",
    "else:\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, \n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm_object,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=current_retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=show_sources.value\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af72d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your final chat display cell to include calendar context\n",
    "if url_input.value and url_input.value.strip() and crawled_content:\n",
    "    chat_messages.append(f\"üåê Using content crawled from: {url_input.value}\")\n",
    "    chat_messages.append(f\"üìÑ Crawled {len(crawled_content.split('--- Page'))} pages\")\n",
    "\n",
    "if calendar_connected and calendar_content:\n",
    "    chat_messages.append(f\"üìÖ Using calendar data with {len(calendar_content.split('Event:'))-1} events\")\n",
    "\n",
    "if file_upload.filepath is None and not (url_input.value and url_input.value.strip()) and not calendar_connected:\n",
    "    normal_no_context_result = qa_chain.invoke({\"question\": question.value})\n",
    "    llm_result = {'result': normal_no_context_result.content}\n",
    "else:\n",
    "    llm_result = qa_chain.invoke({\"query\": question.value})\n",
    "\n",
    "if question.value != '':\n",
    "    if show_sources.value:\n",
    "        sources_result_string = f\"\\nüìö SOURCES ({len(llm_result.get('source_documents', []))} documents):\"\n",
    "        for i, doc in enumerate(llm_result.get('source_documents', [])):\n",
    "            sources_result_string += f\"\\n\\nSource {i+1}:\"\n",
    "            sources_result_string += f\"\\nContent: {doc.page_content}...\"\n",
    "            if hasattr(doc, 'metadata') and doc.metadata:\n",
    "                sources_result_string += f\"\\nMetadata: {doc.metadata}\"\n",
    "    else:\n",
    "        sources_result_string = ''\n",
    "\n",
    "    chat_messages.append(question.value)\n",
    "    chat_messages.append(llm_result['result'] + sources_result_string)\n",
    "\n",
    "mr.Chat(chat_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b38d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
