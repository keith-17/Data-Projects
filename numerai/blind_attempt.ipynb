{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numerapi import NumerAPI\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "import xlsxwriter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c3583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_round_used = 1112\n",
    "data_version = 'v5.0'\n",
    "data_folder = 'data_folder'\n",
    "report_folder = 'reports'\n",
    "skip = False\n",
    "feature_set_chosen = 'midnight'\n",
    "year_horizon = [1, 2, 3]\n",
    "target_column_name = 'target'\n",
    "target_mode = 'single'\n",
    "current_date = '10_10_2025'\n",
    "report_name = f'an_attempt_{current_date}.xlsx'\n",
    "path_to_save_report = f'{report_folder}/{report_name}'\n",
    "\n",
    "folders_list = [data_folder, report_folder]\n",
    "for folder in folders_list:\n",
    "    import os\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "do_cv= False\n",
    "do_master_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca18dd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6cd73c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76099319",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = NumerAPI()\n",
    "api.download_dataset(\n",
    "\tf\"{data_version}/live_example_preds.parquet\",\n",
    "\tf\"{data_folder}/{data_version}/{tournament_round_used}/live_example_round.parquet\",\n",
    "\ttournament_round_used\n",
    ")\n",
    "api.download_dataset(\n",
    "\tf\"{data_version}/validation_example_preds.parquet\",\n",
    "\tf\"{data_folder}/{data_version}/{tournament_round_used}/validation_example_round.parquet\",\n",
    "\ttournament_round_used\n",
    ")\n",
    "api.download_dataset(\n",
    "    f\"{data_version}/features.json\", \n",
    "    f\"{data_folder}/{data_version}/{tournament_round_used}/features.json\"\n",
    ")\n",
    "api.download_dataset(\n",
    "\tf\"{data_version}/train.parquet\",\n",
    "\tf\"{data_folder}/{data_version}/{tournament_round_used}/train.parquet\",\n",
    "\ttournament_round_used\n",
    ")\n",
    "api.download_dataset(\n",
    "\tf\"{data_version}/live_example_preds.parquet\",\n",
    "\tf\"{data_folder}/{data_version}/{tournament_round_used}/live_example_preds.parquet\",\n",
    "\ttournament_round_used\n",
    ")\n",
    "api.download_dataset(\n",
    "\tf\"{data_version}/validation.parquet\",\n",
    "\tf\"{data_folder}/{data_version}/{tournament_round_used}/validation.parquet\",\n",
    "\ttournament_round_used\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8164ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2549107b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_metadata = json.load(open(f\"{data_folder}/{data_version}/{tournament_round_used}/features.json\"))\n",
    "for metadata in feature_metadata:\n",
    "  print(metadata, len(feature_metadata[metadata]))\n",
    "\n",
    "target_set = feature_metadata['targets'].copy()\n",
    "feature_set = feature_metadata[\"feature_sets\"][feature_set_chosen]\n",
    "\n",
    "target_set.remove('target_jeremy_20')\n",
    "target_set.remove('target_jeremy_60')\n",
    "\n",
    "raw_train_df = pd.read_parquet(\n",
    "    f\"{data_folder}/{data_version}/{tournament_round_used}/train.parquet\",\n",
    "    columns=['era'] + feature_set + target_set\n",
    ")\n",
    "\n",
    "raw_validation_df = pd.read_parquet(\n",
    "    f\"{data_folder}/{data_version}/{tournament_round_used}/validation.parquet\",\n",
    "    columns=['era'] + feature_set + target_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697ce1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dadcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df = raw_train_df.copy()\n",
    "sliced_df['week_no'] = (sliced_df['era'].astype(int) - 1) % 52 + 1\n",
    "sliced_df['year_horizon'] = (sliced_df['era'].astype(int) - 1) // 52 + 1\n",
    "sliced_df['era'] = sliced_df['era'].astype(int)\n",
    "\n",
    "sliced_df = sliced_df[sliced_df['year_horizon'].isin(year_horizon)]\n",
    "sliced_df['target'] = sliced_df['target'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e76b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57ee36e3",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5222d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "for objective_chosen in target_set:\n",
    "# for objective_chosen in target_set:\n",
    "    print(f\"Objective chosen: {objective_chosen}\")\n",
    "\n",
    "    train_df = sliced_df.drop(columns=target_set)\n",
    "    test_df = sliced_df[objective_chosen]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_df, test_df, test_size=0.2, random_state=42, stratify=train_df['year_horizon']\n",
    "    )\n",
    "\n",
    "    xgb_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "    try:\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model for target {target_column_name}: {e}\")\n",
    "\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    r2_score_value_xgb = r2_score(y_test, y_pred_xgb)\n",
    "    rmse_value_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    mae_value_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    mse_value_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "\n",
    "    result = pd.Series({\n",
    "        \"target\": objective_chosen,\n",
    "        \"r2_score\": r2_score_value_xgb,\n",
    "        \"rmse\": rmse_value_xgb,\n",
    "        \"mae\": mae_value_xgb,\n",
    "        \"mse\": mse_value_xgb\n",
    "    })\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "results_df = pd.concat(results_list, axis=1).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd06a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e81fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_cv:\n",
    "    cv_obj = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(\n",
    "        xgb_model, train_df, test_df, cv=cv_obj, groups=train_df['year_horizon'], scoring='neg_root_mean_squared_error', n_jobs=-1,\n",
    "    )\n",
    "    print(\"Cross-validation RMSE scores:\", -cv_scores)\n",
    "    print(\"Mean CV RMSE:\", -cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844aa2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df, test_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train your model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "model_fit_summary_df = pd.DataFrame({\n",
    "    \"set\": [\"train\", \"test\"],\n",
    "    \"mse\": [train_mse, test_mse],\n",
    "    \"rmse\": [train_rmse, test_rmse],\n",
    "    \"r2\": [train_r2, test_r2]\n",
    "})\n",
    "\n",
    "model_fit_summary_formatted_df = model_fit_summary_df.style.format({\n",
    "    \"mse\": \"{:.4f}\",\n",
    "    \"rmse\": \"{:.4f}\",\n",
    "    \"r2\": \"{:.4f}\"\n",
    "})\n",
    "\n",
    "print(\"=== REGRESSION PERFORMANCE ===\")\n",
    "print(f\"Training MSE:  {train_mse:.4f}\")\n",
    "print(f\"Testing MSE:   {test_mse:.4f}\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Testing RMSE:  {test_rmse:.4f}\")\n",
    "print(f\"Training RÂ²:   {train_r2:.4f}\")\n",
    "print(f\"Testing RÂ²:    {test_r2:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if train_mse < test_mse * 0.7:  # Train MSE is much lower\n",
    "    print(\"ðŸš¨ OVERFITTING - Model performs much better on training data\")\n",
    "elif train_r2 < 0.6 and test_r2 < 0.6:  # Both RÂ² are low\n",
    "    print(\"ðŸš¨ UNDERFITTING - Model performs poorly on both sets\")\n",
    "else:\n",
    "    print(\"âœ… GOOD FIT - Reasonable performance on both sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7701b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62243886",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e44de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_formatted_df = results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8e1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.concat([y_test.reset_index(), pd.Series(y_pred_xgb, name='predicted')], axis=1)\n",
    "df_compare = df_compare.merge(sliced_df.reset_index()[['id', 'week_no', 'year_horizon']], on='id', how='left')\n",
    "\n",
    "df_compare['error'] = df_compare['target'] - df_compare['predicted']\n",
    "\n",
    "# Calculate percentage error safely (avoid division by zero)\n",
    "df_compare['percentage_error'] = np.where(\n",
    "    df_compare['target'] != 0,\n",
    "    (df_compare['error'] / df_compare['target']) * 100,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Define bins for % error\n",
    "bins = [-np.inf, -10, -5, 5, 10, np.inf]\n",
    "labels = ['Extreme Underforecast', 'Underforecast', 'Accurate', 'Overforecast', 'Extreme Overforecast']\n",
    "\n",
    "# Assign forecast for nonzero targets\n",
    "df_compare.loc[df_compare['target'] != 0, 'forecast'] = pd.cut(\n",
    "    df_compare.loc[df_compare['target'] != 0, 'percentage_error'],\n",
    "    bins=bins,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "# Handle zero-target cases explicitly\n",
    "abs_tol_accurate = 0.01  # adjust based on your data scale\n",
    "abs_tol_extreme = 0.1\n",
    "\n",
    "zero_mask = df_compare['target'] == 0\n",
    "\n",
    "df_compare.loc[zero_mask & (df_compare['predicted'].abs() <= abs_tol_accurate), 'forecast'] = 'Accurate'\n",
    "df_compare.loc[zero_mask & (df_compare['predicted'] > abs_tol_accurate) & (df_compare['predicted'] <= abs_tol_extreme), 'forecast'] = 'Overforecast'\n",
    "df_compare.loc[zero_mask & (df_compare['predicted'] > abs_tol_extreme), 'forecast'] = 'Extreme Overforecast'\n",
    "df_compare.loc[zero_mask & (df_compare['predicted'] < -abs_tol_accurate) & (df_compare['predicted'] >= -abs_tol_extreme), 'forecast'] = 'Underforecast'\n",
    "df_compare.loc[zero_mask & (df_compare['predicted'] < -abs_tol_extreme), 'forecast'] = 'Extreme Underforecast'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883cb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_comparison = df_compare.groupby(['forecast', 'week_no']).agg({'id': 'nunique'}).reset_index().pivot(index='forecast', columns='week_no', values='id').fillna(0).T\n",
    "week_comparison_formatted_df = week_comparison.applymap(lambda x: f\"{x:,}\")\n",
    "week_comparison_formatted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae5bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_summary_df = df_compare.groupby('forecast').agg(**{'count': ('id', 'nunique')})\n",
    "forecast_summary_formatted_df = forecast_summary_df.copy()\n",
    "forecast_summary_formatted_df['population_share'] = forecast_summary_formatted_df['count'] / forecast_summary_formatted_df['count'].sum() * 100\n",
    "forecast_summary_formatted_df['count'] = forecast_summary_formatted_df['count'].apply(lambda x: f\"{x:,}\")\n",
    "forecast_summary_formatted_df['population_share'] = round(forecast_summary_formatted_df['population_share'], 2)\n",
    "forecast_summary_formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce76c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec02a8f",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71369011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all variables ending with '_summary_df'\n",
    "summary_vars = [var for var in globals() if var.endswith('_formatted_df')]\n",
    "\n",
    "# Create a new Excel file\n",
    "with xlsxwriter.Workbook(path_to_save_report) as workbook:\n",
    "    for var in summary_vars:\n",
    "        df = globals()[var]\n",
    "        worksheet = workbook.add_worksheet(var)\n",
    "        # Write column headers\n",
    "        for col_num, col_name in enumerate(df.columns.insert(0, df.index.name or 'index')):\n",
    "            worksheet.write(0, col_num, col_name)\n",
    "        # Write data rows\n",
    "        if isinstance(df, pd.io.formats.style.Styler):\n",
    "            df = df.data\n",
    "        for row_num, (idx, row) in enumerate(df.iterrows(), start=1):\n",
    "            worksheet.write(row_num, 0, idx)\n",
    "            for col_num, value in enumerate(row, start=1):\n",
    "                worksheet.write(row_num, col_num, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b498e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
