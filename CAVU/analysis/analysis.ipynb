{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0951c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import general_utils\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xlsxwriter\n",
    "import io\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446c985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/maran/OneDrive/Documents/Git Profile/Data-Projects/CAVU/raw_data'\n",
    "bookings_sample_file = 'bookings_sample (1).csv'\n",
    "flights_sample_file = 'flights_sample.csv'\n",
    "bookings_sample_path = directory + '/' + bookings_sample_file\n",
    "flights_sample_path = directory + '/' + flights_sample_file\n",
    "final_bookings_file = 'final_bookings_df.csv'\n",
    "directorys_to_make = []\n",
    "csv_directory = 'temp_csv/'\n",
    "reports_directory = 'reports/'\n",
    "os.makedirs(reports_directory, exist_ok=True)\n",
    "os.makedirs(csv_directory, exist_ok=True)\n",
    "use_weekly = True\n",
    "\n",
    "path_to_final_bookings = csv_directory + final_bookings_file\n",
    "\n",
    "car_park_columns = ['carpark1', 'carpark2', 'carpark3', 'carpark4', 'carpark5', 'carpark6', 'carpark7']\n",
    "\n",
    "second_last_ledger_column_names = ['date_occupied', 'terminal', 'carpark_name', 'occupancy','total_revenue', 'bookings_count', 'avg_duration_days', 'min_duration_days', 'avg_lead_time_days', 'min_lead_time_days',\n",
    "       'discounted_bookings', 'discount_ratio', 'cancel_ratio', 'dow', 'month', 'is_weekend', 'flight_date', 'terminal_name', 'total_flights',\n",
    "       'total_passengers', 'intl_flights', 'intl_share', 'europe_share', 'asia_share', 'na_share', 'africa_share', 'holiday_flag', 'weekofyear']\n",
    "       \n",
    "new_second_ledger_column_time_name = 'uc_local_flight_date'\n",
    "second_last_ledger_column_names.append(new_second_ledger_column_time_name)\n",
    "\n",
    "version = 'v0.1.0'\n",
    "\n",
    "naive_split_bool = False\n",
    "\n",
    "if use_weekly:\n",
    "    report_name = \"CAVU_Analysis_Report_{version}_Weekly\"\n",
    "else:\n",
    "    report_name = \"CAVU_Analysis_Report_{version}_Daily\"\n",
    "\n",
    "path_to_report = f\"{reports_directory}/{report_name}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f11a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd514b5a",
   "metadata": {},
   "source": [
    "# 1. Load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bookings_df = pd.read_csv(bookings_sample_path)\n",
    "raw_flights_df = pd.read_csv(flights_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5363733e",
   "metadata": {},
   "source": [
    "## Flights data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = raw_flights_df.copy(deep=True)\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7140614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41979936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3414e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df['destination_airport_name_continent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33023a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69367aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df[flights_df['destination_airport_name_continent'] == 'Unknown Continent']\n",
    "\n",
    "flights_df['flight_number_full_icao'] = flights_df['flight_number_full_icao'].str.upper()\n",
    "flights_df['is_international'] = flights_df['destination_airport_dom_intern'] == 'International'\n",
    "flights_df['is_europe'] = flights_df['destination_airport_name_continent'] == 'Europe'\n",
    "flights_df['is_asia'] = flights_df['destination_airport_name_continent'] == 'Asia'\n",
    "\n",
    "manohar_rows = flights_df['destination_airport_name'].str.contains('Manohar', na=False)\n",
    "flights_df.loc[manohar_rows, 'is_asia'] = True\n",
    "\n",
    "flights_df['is_north_america'] = flights_df['destination_airport_name_continent'] == 'North America'\n",
    "flights_df['is_africa'] = flights_df['destination_airport_name_continent'] == 'Africa'\n",
    "\n",
    "flights_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93907e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da16da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_min_date = flights_df['flight_date'].min()\n",
    "flight_max_date = flights_df['flight_date'].max()\n",
    "\n",
    "print(f\"Flight Date Range: {flight_min_date} to {flight_max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6424f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95348dc8",
   "metadata": {},
   "source": [
    "## Bookings data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_booking_records = raw_bookings_df.shape[0]\n",
    "raw_booking_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3fcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bookings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bookings_df['started_at'].min(), raw_bookings_df['started_at'].max(), raw_bookings_df['created_at'].min(), raw_bookings_df['created_at'].max(), raw_bookings_df['closed_at'].min(), raw_bookings_df['closed_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b668ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bookings_df[raw_bookings_df['created_at'] == '2018-09-09 15:50:21.487'].shape\n",
    "raw_bookings_df[raw_bookings_df.duplicated(subset=['created_at', 'amount'])].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12e9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7419122",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_bookings_df[raw_bookings_df.duplicated(subset=['created_at', 'outbound_flight_no'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60bc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df = raw_bookings_df.copy(deep=True)\n",
    "bookings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afee59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookings_df['booking_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f78a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17caf17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_booking = bookings_df['booking_status'] == 'Confirmed'\n",
    "\n",
    "same_time_period = bookings_df['created_at'].between(flight_min_date, flight_max_date, inclusive='both')\n",
    "\n",
    "bookings_df.loc[:, 'to_keep'] = confirmed_booking & same_time_period\n",
    "\n",
    "bookings_df['to_keep'].fillna(False, inplace=True)\n",
    "\n",
    "filtered_bookings_df = bookings_df[bookings_df['to_keep']].drop(columns=['to_keep'])\n",
    "\n",
    "filtered_bookings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd222f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bookings_df[~filtered_bookings_df['started_at'].astype(str).str.endswith('+00')].shape, filtered_bookings_df[~filtered_bookings_df['closed_at'].astype(str).str.endswith('+00')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bookings_df['created_at'] = pd.to_datetime(filtered_bookings_df['created_at'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_bookings_df['started_at'] = pd.to_datetime(filtered_bookings_df['started_at'], format='%Y-%m-%d %H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_bookings_df['closed_at'] = pd.to_datetime(filtered_bookings_df['closed_at'], format='%Y-%m-%d %H:%M:%S%z', errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "filtered_bookings_df['is_discount_booking'] = filtered_bookings_df['is_discount_booking'].astype(bool)\n",
    "filtered_bookings_df['booking_status'] = filtered_bookings_df['booking_status'].astype('category')\n",
    "\n",
    "filtered_bookings_df['flight_date'] = pd.to_datetime(filtered_bookings_df['started_at'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "filtered_bookings_df['created_at_dt'] = pd.to_datetime(filtered_bookings_df['created_at'], errors='coerce')\n",
    "filtered_bookings_df['started_at_dt'] = pd.to_datetime(filtered_bookings_df['started_at'], errors='coerce')\n",
    "filtered_bookings_df['closed_at_dt'] = pd.to_datetime(filtered_bookings_df['closed_at'], errors='coerce')\n",
    "\n",
    "filtered_bookings_df['parking_duration_days'] = (filtered_bookings_df['closed_at_dt'] - filtered_bookings_df['started_at_dt']).dt.days\n",
    "filtered_bookings_df['lead_time_days'] = (filtered_bookings_df['started_at_dt'] - filtered_bookings_df['created_at_dt']).dt.days\n",
    "\n",
    "filtered_bookings_df['flight_number_full_icao'] = filtered_bookings_df['outbound_flight_no'].str.upper()\n",
    "\n",
    "filtered_bookings_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4a066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for bookings_df\n",
    "nan_terminal_records_bookings = bookings_df[bookings_df['terminal'].isnull()].shape[0]\n",
    "total_raw_booking_records_bookings = bookings_df.shape[0]\n",
    "nan_terminal_percentage_bookings = (nan_terminal_records_bookings / total_raw_booking_records_bookings) * 100\n",
    "total_records_to_keep_bookings = bookings_df[bookings_df['to_keep']].shape[0]\n",
    "\n",
    "print(\"bookings_df:\")\n",
    "print(f\"  Total records: {total_raw_booking_records_bookings}\")\n",
    "print(f\"  Records to keep: {total_records_to_keep_bookings}\")\n",
    "print(f\"  NaN 'terminal': {nan_terminal_records_bookings}\")\n",
    "print(f\"  % NaN 'terminal': {nan_terminal_percentage_bookings:.2f}%\")\n",
    "print(f\"  NaN 'terminal' & to_keep: {bookings_df[bookings_df['terminal'].isnull() & bookings_df['to_keep']].shape[0]}\")\n",
    "print(f\"  % NaN 'terminal' & to_keep: {(bookings_df[bookings_df['terminal'].isnull() & bookings_df['to_keep']].shape[0] / total_records_to_keep_bookings) * 100:.2f}%\")\n",
    "\n",
    "# Analysis for filtered_bookings_df\n",
    "nan_terminal_records_filtered = filtered_bookings_df[filtered_bookings_df['terminal'].isnull()].shape[0] if 'terminal' in filtered_bookings_df.columns else 0\n",
    "total_raw_booking_records_filtered = filtered_bookings_df.shape[0]\n",
    "nan_terminal_percentage_filtered = (nan_terminal_records_filtered / total_raw_booking_records_filtered) * 100 if total_raw_booking_records_filtered > 0 else 0\n",
    "\n",
    "print(\"\\nfiltered_bookings_df:\")\n",
    "print(f\"  Total records: {total_raw_booking_records_filtered}\")\n",
    "print(f\"  NaN 'terminal': {nan_terminal_records_filtered}\")\n",
    "print(f\"  % NaN 'terminal': {nan_terminal_percentage_filtered:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a00f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd27a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_terminal_bookings_df = filtered_bookings_df[~filtered_bookings_df['terminal'].isna()]\n",
    "\n",
    "missing_terminal_bookings_df = filtered_bookings_df[filtered_bookings_df['terminal'].isna()]#.rename(columns={'outbound_flight_no': 'flight_number_full_icao'})\n",
    "\n",
    "required_flight_cols = ['flight_number_full_icao', 'terminal_name', 'flight_date']\n",
    "\n",
    "recovered_terminal_flights_df = missing_terminal_bookings_df.merge(flights_df[required_flight_cols], on=['flight_number_full_icao', 'flight_date'], how='left')\n",
    "\n",
    "recovered_terminal_flights_df = recovered_terminal_flights_df.drop(columns=['terminal']).rename(columns={'terminal_name': 'terminal'})\n",
    "\n",
    "bookings_validated_df = pd.concat([has_terminal_bookings_df, recovered_terminal_flights_df], ignore_index=True)\n",
    "\n",
    "nan_terminals = bookings_validated_df['terminal'].isna().sum()\n",
    "\n",
    "bookings_validated_df = bookings_validated_df[bookings_validated_df['terminal'].notna()]\n",
    "\n",
    "# bookings_validated_df['terminal'] = bookings_validated_df['terminal'].fillna('Unknown')\n",
    "\n",
    "bookings_validated_df.shape, bookings_df.shape, nan_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564da77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bookings_df = bookings_validated_df.melt(\n",
    "    id_vars=[c for c in bookings_validated_df.columns if c not in car_park_columns],\n",
    "    value_vars=car_park_columns,\n",
    "    var_name=\"carpark_name\",\n",
    "    value_name=\"is_carpark\"\n",
    ")\n",
    "\n",
    "final_bookings_df = final_bookings_df[(final_bookings_df['is_carpark'] == 1).drop(columns=['is_carpark'])].reset_index(drop=True)\n",
    "final_bookings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900b2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdddcd30",
   "metadata": {},
   "source": [
    "# 2. Aggregate for features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce4520",
   "metadata": {},
   "source": [
    "## Daily Bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute normalized dates CORRECT AND SLIGHTLY FASTER\n",
    "starts = final_bookings_df['started_at_dt'].dt.normalize()\n",
    "ends = final_bookings_df['closed_at_dt'].dt.normalize()\n",
    "\n",
    "# Generate date ranges using list comprehension (faster than apply)\n",
    "final_bookings_df['occupancy_days'] = [\n",
    "    pd.date_range(s, e, freq='D') \n",
    "    for s, e in zip(starts, ends)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf3599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_to_final_bookings):\n",
    "    print(\"Loading final_bookings_df from CSV...\")\n",
    "    final_bookings_exploded_df = pd.read_csv(path_to_final_bookings)\n",
    "else:\n",
    "    print(\"Generating final_bookings_exploded_df and saving to CSV...\")\n",
    "    final_bookings_exploded_df = final_bookings_df.explode('occupancy_days').rename(columns={'occupancy_days': 'date_occupied'})\n",
    "    final_bookings_exploded_df.to_csv(path_to_final_bookings, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578ef3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_bookings_df = final_bookings_exploded_df.groupby(['date_occupied', 'terminal', 'carpark_name']).agg(**{\n",
    "    'occupancy': ('date_occupied', 'count'),\n",
    "    'total_revenue': ('amount', 'sum'),\n",
    "    'bookings_count': ('amount', 'size'),\n",
    "    'avg_duration_days': ('parking_duration_days', 'mean'),\n",
    "    'min_duration_days': ('parking_duration_days', 'min'),\n",
    "    'avg_lead_time_days': ('lead_time_days', 'mean'),\n",
    "    'min_lead_time_days': ('lead_time_days', 'min'),\n",
    "    'discounted_bookings': ('is_discount_booking', 'sum'),\n",
    "    'discount_ratio': ('is_discount_booking', 'mean'),\n",
    "    'cancel_ratio': ('booking_status', lambda x: (x == 'canceled').mean())\n",
    "}).reset_index()\n",
    "\n",
    "daily_bookings_df['dow'] = pd.to_datetime(daily_bookings_df['date_occupied']).dt.dayofweek\n",
    "daily_bookings_df['month'] = pd.to_datetime(daily_bookings_df['date_occupied']).dt.month\n",
    "daily_bookings_df['is_weekend'] = daily_bookings_df['dow'].isin([5, 6]).astype(int)\n",
    "\n",
    "daily_bookings_df.rename(columns={'date_occupied': new_second_ledger_column_time_name}, inplace=True)\n",
    "\n",
    "daily_bookings_df[new_second_ledger_column_time_name] = pd.to_datetime(daily_bookings_df[new_second_ledger_column_time_name], errors='coerce')\n",
    "\n",
    "daily_bookings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fa1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e9000c7",
   "metadata": {},
   "source": [
    "## Flights features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e63d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_agg_df = flights_df.groupby(['flight_date', 'terminal_name']).agg(**{\n",
    "    'total_flights': ('flight_number_full_icao', 'nunique'),\n",
    "    'total_passengers': ('pax_quantity', 'sum'),\n",
    "    'intl_flights': ('is_international', 'sum'),\n",
    "    'intl_pax': ('pax_quantity', lambda x: x[flights_df.loc[x.index, 'is_international'] == 1].sum()),\n",
    "    'europe_pax': ('pax_quantity', lambda x: x[flights_df.loc[x.index, 'is_europe'] == True].sum()),\n",
    "    'asia_pax': ('pax_quantity', lambda x: x[flights_df.loc[x.index, 'is_asia'] == True].sum()),\n",
    "    'na_pax': ('pax_quantity', lambda x: x[flights_df.loc[x.index, 'is_north_america'] == True].sum()),\n",
    "    'africa_pax': ('pax_quantity', lambda x: x[flights_df.loc[x.index, 'is_africa'] == True].sum()),\n",
    "}).reset_index()\n",
    "\n",
    "flights_agg_df['flight_date'] = pd.to_datetime(flights_agg_df['flight_date'])\n",
    "\n",
    "flights_agg_df['intl_share'] = flights_agg_df['intl_pax'] / flights_agg_df['total_passengers']\n",
    "flights_agg_df['europe_share'] = flights_agg_df['europe_pax'] / flights_agg_df['total_passengers']\n",
    "flights_agg_df['asia_share'] = flights_agg_df['asia_pax'] / flights_agg_df['total_passengers']\n",
    "flights_agg_df['na_share'] = flights_agg_df['na_pax'] / flights_agg_df['total_passengers']\n",
    "flights_agg_df['africa_share'] = flights_agg_df['africa_pax'] / flights_agg_df['total_passengers']\n",
    "\n",
    "uk_holidays = holidays.UnitedKingdom(years=range(2017, 2026))\n",
    "flights_agg_df['holiday_flag'] = flights_agg_df['flight_date'].dt.date.astype('datetime64[ns]').isin(uk_holidays).astype(int)\n",
    "\n",
    "flights_agg_df['weekofyear'] = flights_agg_df['flight_date'].dt.isocalendar().week\n",
    "\n",
    "flights_agg_df['financial_quarter'] = flights_agg_df['flight_date'].dt.quarter\n",
    "\n",
    "flights_agg_df.rename(columns={'flight_date': new_second_ledger_column_time_name}, inplace=True)\n",
    "\n",
    "flights_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab3988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_bookings_df.rename(columns={'date_occupied': new_second_ledger_column_time_name}, inplace=True)\n",
    "\n",
    "flights_agg_df['uc_local_flight_date'].min(), flights_agg_df['uc_local_flight_date'].max(), flights_df['flight_date'].min(), flights_df['flight_date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573657e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13b0a632",
   "metadata": {},
   "source": [
    "## Create Master Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = daily_bookings_df.merge(\n",
    "    flights_agg_df,\n",
    "    left_on=[new_second_ledger_column_time_name, 'terminal'],\n",
    "    right_on=[new_second_ledger_column_time_name, 'terminal_name'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123c6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = daily_bookings_df.merge(flights_agg_df, left_on=[new_second_ledger_column_time_name, 'terminal'], right_on=[new_second_ledger_column_time_name, 'terminal_name'], how='left')\n",
    "filtered_dates_rows = merged_df[new_second_ledger_column_time_name].between(flight_min_date, flight_max_date, inclusive='both')\n",
    "master_df = merged_df.loc[filtered_dates_rows, :].drop(columns=['terminal_name'])\n",
    "\n",
    "master_df.shape, merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b246f8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df70846b",
   "metadata": {},
   "source": [
    "## Filtering, cleaning and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d09eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nan_cols = [\n",
    "    'total_flights', 'total_passengers', 'intl_share', 'europe_share', 'asia_share',\n",
    "    'na_share', 'africa_share', 'holiday_flag', 'dow', 'month', 'is_weekend'\n",
    "]\n",
    "\n",
    "columns_to_encode = ['terminal', 'carpark_name', 'uc_local_flight_date']\n",
    "\n",
    "encoded_columns_list = [f\"{col}_num\" for col in columns_to_encode]\n",
    "\n",
    "master_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d0e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df[to_nan_cols] = master_df[to_nan_cols].fillna(0)\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    master_df[f\"{col}_num\"] = master_df[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02292a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53542813",
   "metadata": {},
   "source": [
    "## Weekly aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df = master_df.groupby(['weekofyear', 'terminal', 'carpark_name']).agg({\n",
    "    'occupancy': 'sum',\n",
    "    'total_revenue': 'sum',\n",
    "    'bookings_count': 'sum',\n",
    "    'avg_duration_days': 'mean',\n",
    "    'min_duration_days': 'min',\n",
    "    'avg_lead_time_days': 'mean',\n",
    "    'min_lead_time_days': 'min',\n",
    "    'discounted_bookings': 'sum',\n",
    "    'discount_ratio': 'mean',\n",
    "    'cancel_ratio': 'mean',\n",
    "    'dow': 'first',\n",
    "    'month': 'first',\n",
    "    'is_weekend': 'first',\n",
    "    'total_flights': 'sum',\n",
    "    'total_passengers': 'sum',\n",
    "    'intl_flights': 'sum',\n",
    "    'intl_share': 'mean',\n",
    "    'europe_share': 'mean',\n",
    "    'asia_share': 'mean',\n",
    "    'na_share': 'mean',\n",
    "    'africa_share': 'mean',\n",
    "    'holiday_flag': 'max',\n",
    "    'carpark_name_num': 'first',\n",
    "    'financial_quarter': 'first',\n",
    "    'terminal_num': 'first'\n",
    "}).reset_index().drop(columns=['terminal','carpark_name'])\n",
    "\n",
    "weekly_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f02439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e42215d",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffa9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b91908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0a89bac",
   "metadata": {},
   "source": [
    "# 3. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ac6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09da3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4af404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_flights_df = flights_df[flights_df['flight_date'].between(flight_min_date, flight_max_date, inclusive='both')].sort_values(by=['flight_date', 'terminal_name'])\n",
    "sliced_flights_df['weekofyear'] = pd.to_datetime(sliced_flights_df['flight_date']).dt.isocalendar().week\n",
    "sliced_flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417476d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee34464",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sliced_flights_df.groupby(['weekofyear','terminal_name']).agg({'pax_quantity': 'sum'}).unstack().droplevel(level=0, axis=1).plot(\n",
    "    kind='line', title='Weekly Passenger Count Over Time', figsize=(12,6)\n",
    ")\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "lines = ax.get_lines()\n",
    "if len(lines) > 1:\n",
    "    lines[1].set_linestyle('--')\n",
    "\n",
    "for q, label in zip([13, 26, 39, 52], ['Q1', 'Q2', 'Q3', 'Q4']):\n",
    "    ax.axvline(x=q, color='gray', linestyle=':', linewidth=1)\n",
    "    ax.text(q, ax.get_ylim()[1], label, color='gray', ha='center', va='top', fontsize=10)\n",
    "\n",
    "    ax.set_ylabel('Outbound Passengers')\n",
    "    ax.set_xlabel('Week of Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdb348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f869447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "for terminal in master_df['terminal'].unique():\n",
    "    subset = master_df[master_df['terminal'] == terminal]\n",
    "    avg_by_week = subset.groupby('weekofyear')['occupancy'].mean()\n",
    "    plt.plot(avg_by_week.index, avg_by_week.values, label=terminal)\n",
    "\n",
    "plt.xlabel('Week of Year')\n",
    "plt.ylabel('Average Occupancy')\n",
    "plt.title('Average Occupancy per Terminal per Week - 2023')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.gca().set_ylim(bottom=0)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "for q in [13, 26, 39, 52]:\n",
    "    plt.axvline(x=q, color='gray', linestyle=':', linewidth=1)\n",
    "    plt.text(q, plt.gca().get_ylim()[1], f'Q{[13,26,39,52].index(q)+1}', color='gray', ha='center', va='top', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6269444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_occupancy_per_carpark = master_df.groupby(['terminal', 'carpark_name','weekofyear'])['occupancy'].mean().unstack().fillna(0)\n",
    "\n",
    "ave_occupancy_per_carpark\n",
    "\n",
    "for terminal in master_df['terminal'].unique():\n",
    "    subset = master_df[master_df['terminal'] == terminal]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for carpark in subset['carpark_name'].unique():\n",
    "        carpark_data = subset[subset['carpark_name'] == carpark].groupby(new_second_ledger_column_time_name)['occupancy'].mean()\n",
    "        plt.plot(carpark_data.index, carpark_data.values, label=f\"{carpark}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Occupancy')\n",
    "    plt.title(f'Occupancy Over Time - Terminal {terminal}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da6015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c8a9d4c",
   "metadata": {},
   "source": [
    "# 4. Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ff7bf",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fad5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69b9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd037e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_view_df = master_df.groupby(['terminal', 'carpark_name']).agg({'occupancy': ['sum', 'mean']})\n",
    "occupancy_view_df = occupancy_view_df.applymap(lambda x: \"{:,}\".format(int(x)))\n",
    "occupancy_view_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6e75b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.groupby(['financial_quarter', 'terminal']).agg({'occupancy': ['sum', 'mean']}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae2542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_split_list = ['financial_quarter', 'terminal_num', 'carpark_name_num']\n",
    "\n",
    "if use_weekly:\n",
    "    print(\"Using weekly data for train-test split...\")\n",
    "    master_df_sorted = weekly_df.sample(frac=1).reset_index(drop=True)\n",
    "else:\n",
    "    # Sort by date\n",
    "    master_df_sorted = master_df.drop(columns=columns_to_encode).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "group_indices_series = master_df_sorted.groupby(group_split_list).ngroup()\n",
    "\n",
    "if naive_split_bool:\n",
    "    # 70/30 split\n",
    "    split_idx = int(len(master_df_sorted) * 0.7)\n",
    "    train_df = master_df_sorted.iloc[:split_idx]\n",
    "    test_df = master_df_sorted.iloc[split_idx:]\n",
    "\n",
    "else:\n",
    "    print(\"Using GroupShuffleSplit for train-test split...\")\n",
    "    gss = GroupShuffleSplit(n_splits=2, test_size=0.5, random_state=42)\n",
    "\n",
    "    for train_idx, test_idx in gss.split(master_df_sorted, groups = group_indices_series):\n",
    "        train_df = master_df_sorted.iloc[train_idx]\n",
    "        test_df = master_df_sorted.iloc[test_idx]\n",
    "\n",
    "# Feature train target train, feature test target test\n",
    "X_train = train_df.drop(columns=['occupancy'])\n",
    "y_train = train_df['occupancy']\n",
    "X_test = test_df.drop(columns=['occupancy'])\n",
    "y_test = test_df['occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19933a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b28a81e",
   "metadata": {},
   "source": [
    "## Trial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "r2_score_value_lr = r2_score(y_test, y_pred_lr)\n",
    "rmse_value_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_value_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "mse_value_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"R2 Score: {r2_score_value_lr:.4f}\")\n",
    "print(f\"RMSE: {rmse_value_lr:.2f}\")\n",
    "print(f\"MAE: {mae_value_lr:.2f}\")\n",
    "print(f\"MSE: {mse_value_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dceb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06cd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target series for ARIMA\n",
    "train_target = train_df['occupancy']\n",
    "test_target = test_df['occupancy']\n",
    "\n",
    "# Fit ARIMA model (simple order, can be tuned)\n",
    "arima_model = ARIMA(train_target, order=(1,1,1))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = arima_result.forecast(steps=len(test_target))\n",
    "y_pred_arima = forecast.values\n",
    "y_test_arima = test_target.values\n",
    "\n",
    "# Metrics\n",
    "r2_arima = r2_score(y_test_arima, y_pred_arima)\n",
    "rmse_arima = np.sqrt(mean_squared_error(y_test_arima, y_pred_arima))\n",
    "mae_arima = mean_absolute_error(y_test_arima, y_pred_arima)\n",
    "mse_arima = mean_squared_error(y_test_arima, y_pred_arima)\n",
    "\n",
    "print(f\"ARIMA R2 Score: {r2_arima:.4f}\")\n",
    "print(f\"ARIMA RMSE: {rmse_arima:.2f}\")\n",
    "print(f\"ARIMA MAE: {mae_arima:.2f}\")\n",
    "print(f\"ARIMA MSE: {mse_arima:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e1ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194cd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_all = False\n",
    "no_cv_bool = True\n",
    "cv_folds = 3\n",
    "\n",
    "# Compute correlation of features with target\n",
    "# Ensure feature_columns is defined and accessible\n",
    "feature_columns = [col for col in train_df.columns if col not in ['occupancy']]\n",
    "\n",
    "corrs = train_df[feature_columns + ['occupancy']].corr()['occupancy'].drop('occupancy')\n",
    "corrs_abs = corrs.abs().sort_values(ascending=False)\n",
    "\n",
    "# Select top N features (e.g., top 8)\n",
    "top_n = 8\n",
    "selected_features = corrs_abs.head(top_n).index.tolist()\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "if use_all:\n",
    "    selected_features = feature_columns\n",
    "\n",
    "if no_cv_bool:\n",
    "    cv_folds=[(slice(None), slice(None))]\n",
    "\n",
    "# Use selected features for training\n",
    "X_train_corr = train_df[selected_features].fillna(0)\n",
    "X_test_corr = test_df[selected_features].fillna(0)\n",
    "\n",
    "# Fit model\n",
    "rf_corr = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [10, 20, 100, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_corr, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train_corr, y_train)\n",
    "\n",
    "rf_corr = grid_search.best_estimator_\n",
    "rf_corr.fit(X_train_corr, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_corr = rf_corr.predict(X_test_corr)\n",
    "\n",
    "# Metrics\n",
    "r2_rf_corr = r2_score(y_test, y_pred_rf_corr)\n",
    "rmse_rf_corr = np.sqrt(mean_squared_error(y_test, y_pred_rf_corr))\n",
    "mae_rf_corr = mean_absolute_error(y_test, y_pred_rf_corr)\n",
    "mse_rf_corr = mean_squared_error(y_test, y_pred_rf_corr)\n",
    "\n",
    "print(f\"Random Forest (Corr) R2 Score: {r2_rf_corr:.4f}\")\n",
    "print(f\"Random Forest (Corr) RMSE: {rmse_rf_corr:.2f}\")\n",
    "print(f\"Random Forest (Corr) MAE: {mae_rf_corr:.2f}\")\n",
    "print(f\"Random Forest (Corr) MSE: {mse_rf_corr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cac33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf_corr.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': X_train_corr.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(importances_df)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances_df['Feature'], importances_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest - Feature Importances')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all features except 'occupancy'\n",
    "all_features = [col for col in train_df.columns if col != 'occupancy']\n",
    "X_train_pca = train_df[all_features].fillna(0)\n",
    "X_test_pca = test_df[all_features].fillna(0)\n",
    "\n",
    "pca_svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('svr', SVR(kernel='rbf', C=1.0, epsilon=0.2))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': list(range(2, min(len(all_features), 10))),\n",
    "    'svr__C': [0.1, 1.0, 10.0],\n",
    "    'svr__epsilon': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "grid_search_pca_svm = GridSearchCV(\n",
    "    pca_svm_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_pca_svm.fit(X_train_pca, y_train)\n",
    "pca_svm_pipeline = grid_search_pca_svm.best_estimator_\n",
    "print(\"Best params:\", grid_search_pca_svm.best_params_)\n",
    "\n",
    "pca_svm_pipeline.fit(X_train_pca, y_train)\n",
    "y_pred_pca_svm = pca_svm_pipeline.predict(X_test_pca)\n",
    "\n",
    "# Metrics\n",
    "r2_pca_svm = r2_score(y_test, y_pred_pca_svm)\n",
    "rmse_pca_svm = np.sqrt(mean_squared_error(y_test, y_pred_pca_svm))\n",
    "mae_pca_svm = mean_absolute_error(y_test, y_pred_pca_svm)\n",
    "mse_pca_svm = mean_squared_error(y_test, y_pred_pca_svm)\n",
    "\n",
    "print(f\"PCA+SVM R2 Score: {r2_pca_svm:.4f}\")\n",
    "print(f\"PCA+SVM RMSE: {rmse_pca_svm:.2f}\")\n",
    "print(f\"PCA+SVM MAE: {mae_pca_svm:.2f}\")\n",
    "print(f\"PCA+SVM MSE: {mse_pca_svm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9d651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3347bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xgb_grid_search'\n",
    "\n",
    "cross_validate = False\n",
    "\n",
    "if cross_validate:\n",
    "    cv_folds = 3\n",
    "else:\n",
    "    cv_folds = [(slice(None), slice(None))]\n",
    "\n",
    "\n",
    "# pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    (f'{model_name}', XGBRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# parameter grid (all combinations tested)\n",
    "param_grid = {\n",
    "    f'{model_name}__n_estimators': [100, 300, 500],\n",
    "    f'{model_name}__max_depth': [3, 6, 9],\n",
    "    f'{model_name}__learning_rate': [0.01, 0.05, 0.1],\n",
    "    f'{model_name}__subsample': [0.8, 1.0],\n",
    "    f'{model_name}__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "xgb_pipeline = GridSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',  # or r2, etc.\n",
    "    cv=cv_folds,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb_array = xgb_pipeline.predict(X_test)\n",
    "\n",
    "xgb_ledger_df = pd.concat(\n",
    "\t[X_test.copy(deep=True), \n",
    "    pd.DataFrame({'model_prediction': y_pred_xgb_array}\n",
    "    , index=X_test.index)],\n",
    "\taxis=1\n",
    ")\n",
    "\n",
    "xgb_ledger_df['mae'] = np.abs(xgb_ledger_df['model_prediction'] - y_test)\n",
    "xgb_ledger_df['mse'] = (xgb_ledger_df['model_prediction'] - y_test)**2\n",
    "xgb_ledger_df['rmse'] = np.sqrt(xgb_ledger_df['mse'])\n",
    "xgb_ledger_df['r2'] = 1 - (xgb_ledger_df['mse'] / np.var(y_test))\n",
    "\n",
    "xgb_ledger_df = pd.concat(\n",
    "    [\n",
    "        xgb_ledger_df,\n",
    "        y_test\n",
    "    ],\n",
    "    axis=1\n",
    ").rename(columns={'occupancy':'ground_truth'})\n",
    "\n",
    "bins = [-np.inf, -15, -5, 5, 15, np.inf]\n",
    "accuracy_labels = ['Extreme Underforecast', 'Underforecast', 'Correct', 'Overforecast', 'Extreme Overforecast']\n",
    "xgb_ledger_df['raw_difference'] = xgb_ledger_df['model_prediction'] - xgb_ledger_df['ground_truth']\n",
    "xgb_ledger_df['perc_difference'] = xgb_ledger_df['raw_difference'] / xgb_ledger_df['ground_truth'] * 100\n",
    "\n",
    "xgb_ledger_df['accuracy_bin'] = pd.cut(xgb_ledger_df['perc_difference'], bins=bins, labels=accuracy_labels)\n",
    "\n",
    "# Metrics\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb_array)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb_array))\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb_array)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb_array)\n",
    "\n",
    "print(f\"XGBoost R2 Score: {r2_xgb:.4f}\")\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "print(f\"XGBoost MSE: {mse_xgb:.2f}\")\n",
    "\n",
    "print(\"Best params (Grid):\", xgb_pipeline.best_params_)\n",
    "print(\"Best score (Grid):\", xgb_pipeline.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229b24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77faecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "calcs_dict = {\n",
    "    'mae': 'mean',\n",
    "    'mse': 'mean',\n",
    "    'rmse': 'mean',\n",
    "    'r2': 'mean',\n",
    "    'model_prediction':'count'\n",
    "}\n",
    "\n",
    "xgb_result_df_overall = xgb_ledger_df.groupby(['terminal_num']).agg({**calcs_dict}).reset_index()\n",
    "\n",
    "xgb_result_df_overall['carpark_name_num'] = 'All Terminals'\n",
    "\n",
    "xgb_result_df_general = xgb_ledger_df.groupby(['terminal_num', 'carpark_name_num']).agg({**calcs_dict}).reset_index()\n",
    "xgb_result_df_general['carpark_name_num'] += 1\n",
    "\n",
    "xgb_result_financial_df = xgb_ledger_df.groupby(['financial_quarter', 'terminal_num', 'carpark_name_num']).agg({**calcs_dict}).reset_index()\n",
    "xgb_result_financial_df['carpark_name_num'] += 1\n",
    "xgb_result_financial_df['terminal_num'] += 1\n",
    "\n",
    "xgb_result_df = pd.concat([xgb_result_df_overall, xgb_result_df_general, xgb_result_financial_df]\n",
    "                          ).fillna({'financial_quarter': 'Year Round'}\n",
    "                                   ).rename(\n",
    "                                       columns={'terminal_num': 'Terminal Number',\n",
    "                                                 'carpark_name_num': 'Carpark Name', \n",
    "                                                 'financial_quarter': 'Financial Quarter', \n",
    "                                                 'model_prediction': 'Records Used'})\n",
    "xgb_result_df['Terminal Number'] += 1\n",
    "xgb_result_df.head()\n",
    "\n",
    "accuracy_bin_zip = {'accuracy_bin': 'value_counts'}\n",
    "\n",
    "# Combine two dictionaries\n",
    "combined_dict = {**accuracy_bin_zip, **calcs_dict}\n",
    "xgb_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba080f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967da301",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ledger_df.groupby('weekofyear').agg({\n",
    "    'mae': 'mean',\n",
    "    \n",
    "}).plot(kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aef03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_bin_df = xgb_ledger_df.groupby(['financial_quarter', 'terminal_num', 'carpark_name_num']).agg(**{\n",
    "    'Record Count': ('accuracy_bin', 'value_counts')\n",
    "})\n",
    "\n",
    "accuracy_bin_df['Percentage'] = accuracy_bin_df.groupby(['financial_quarter', 'terminal_num', 'carpark_name_num'])['Record Count'].transform(lambda x: x / x.sum() * 100)\n",
    "accuracy_bin_df['Percentage'] = accuracy_bin_df['Percentage'].replace(np.nan, np.inf)\n",
    "\n",
    "columns_to_agg = ['financial_quarter', 'terminal_num', 'carpark_name_num']\n",
    "to_append =  accuracy_bin_df.reset_index().groupby(['accuracy_bin', 'financial_quarter']).agg({'Record Count': 'sum', 'Percentage': 'mean'}).reset_index()\n",
    "# to_append['terminal_num'] += 1\n",
    "# to_append['carpark_name_num'] += 1\n",
    "to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d43182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b92b10",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data (use all features except 'occupancy')\n",
    "nn_features = [col for col in X_train.columns if col != 'occupancy']\n",
    "scaler = StandardScaler()\n",
    "X_train_nn = scaler.fit_transform(X_train[nn_features].fillna(0))\n",
    "X_test_nn = scaler.transform(X_test[nn_features].fillna(0))\n",
    "\n",
    "# Build neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_nn.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Early stopping for best results\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(\n",
    "    X_train_nn, y_train.values,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_nn = model.predict(X_test_nn).flatten()\n",
    "\n",
    "# Metrics\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "rmse_nn = mean_squared_error(y_test, y_pred_nn, squared=False)\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network R2 Score: {r2_nn:.4f}\")\n",
    "print(f\"Neural Network RMSE: {rmse_nn:.2f}\")\n",
    "print(f\"Neural Network MAE: {mae_nn:.2f}\")\n",
    "print(f\"Neural Network MSE: {mse_nn:.2f}\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Neural Network Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e473e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714035ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'Model': ['Linear Regression', 'Random Forest (Corr)', 'XGBoost', 'ARIMA', 'PCA+SVM'],\n",
    "    'R2 Score': [r2_score_value_lr, r2_rf_corr, r2_xgb, r2_arima, r2_pca_svm],\n",
    "    'RMSE': [rmse_value_lr, rmse_rf_corr, rmse_xgb, rmse_arima, rmse_pca_svm],\n",
    "    'MAE': [mae_value_lr, mae_rf_corr, mae_xgb, mae_arima, mae_pca_svm],\n",
    "    'MSE': [mse_value_lr, mse_rf_corr, mse_xgb, mse_arima, mse_pca_svm]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b1bce7",
   "metadata": {},
   "source": [
    "# 5. Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23422f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the metrics, excluding ARIMA\n",
    "metrics_df_no_arima = metrics_df[metrics_df['Model'] != 'ARIMA'].copy()\n",
    "\n",
    "# Rename Linear Regression to (Baseline) Linear Regression\n",
    "metrics_df_no_arima.loc[metrics_df_no_arima['Model'] == 'Linear Regression', 'Model'] = '(Baseline) Linear Regression'\n",
    "\n",
    "# Use a color palette for bars\n",
    "bar_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "metrics_to_plot = ['R2 Score', 'RMSE', 'MAE', 'MSE']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, metric in zip(axes.flatten(), metrics_to_plot):\n",
    "    bars = metrics_df_no_arima.plot(\n",
    "        x='Model', y=metric, kind='barh', ax=ax, legend=False, color=bar_colors[:len(metrics_df_no_arima)]\n",
    "    )\n",
    "    ax.set_title(f'{metric} Comparison (Excluding ARIMA)')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    for x in ax.get_xticks():\n",
    "        ax.axvline(x=x, color='lightgray', linestyle=':', linewidth=0.7, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting ARIMA separately\n",
    "metrics_arima = metrics_df[metrics_df['Model'] == 'ARIMA'].copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(15, 5))\n",
    "for ax, metric in zip(axes, metrics_to_plot):\n",
    "    ax.barh(metrics_arima['Model'], metrics_arima[metric], color=bar_colors[0])\n",
    "    ax.set_title(f'ARIMA - {metric}')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    for x in ax.get_xticks():\n",
    "        ax.axvline(x=x, color='lightgray', linestyle=':', linewidth=0.7, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d165a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc738edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Linear Regression to (Baseline) Linear Regression\n",
    "metrics_df_plot = metrics_df.copy()\n",
    "metrics_df_plot.loc[metrics_df_plot['Model'] == 'Linear Regression', 'Model'] = '(Baseline) Linear Regression'\n",
    "\n",
    "# Use a color palette for bars\n",
    "bar_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "metrics_to_plot = ['R2 Score', 'RMSE', 'MAE', 'MSE']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, metric in zip(axes.flatten(), metrics_to_plot):\n",
    "    bars = metrics_df_plot.plot(\n",
    "        x='Model', y=metric, kind='barh', ax=ax, legend=False, color=bar_colors[:len(metrics_df_plot)]\n",
    "    )\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    for x in ax.get_xticks():\n",
    "        ax.axvline(x=x, color='lightgray', linestyle=':', linewidth=0.7, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58070bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter metrics_df to display only Linear Regression, XGBoost, and Random Forest (Corr)\n",
    "metrics_df_selected = metrics_df[metrics_df['Model'].isin(['Linear Regression', 'XGBoost', 'Random Forest (Corr)'])].copy()\n",
    "\n",
    "# Rename Linear Regression to (Baseline) Linear Regression for clarity\n",
    "metrics_df_selected.loc[metrics_df_selected['Model'] == 'Linear Regression', 'Model'] = '(Baseline) Linear Regression'\n",
    "\n",
    "# Use a color palette for bars\n",
    "bar_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "metrics_to_plot = ['R2 Score', 'RMSE', 'MAE', 'MSE']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ax, metric in zip(axes.flatten(), metrics_to_plot):\n",
    "    bars = metrics_df_selected.plot(\n",
    "        x='Model', y=metric, kind='barh', ax=ax, legend=False, color=bar_colors[:len(metrics_df_selected)]\n",
    "    )\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    for x in ax.get_xticks():\n",
    "        ax.axvline(x=x, color='lightgray', linestyle=':', linewidth=0.7, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c20ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc26e80",
   "metadata": {},
   "source": [
    "# END - Save report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel file and add worksheets for each graphic\n",
    "output = io.BytesIO()\n",
    "workbook = xlsxwriter.Workbook(output, {'in_memory': True})\n",
    "\n",
    "# 1. Weekly Passenger Count Over Time\n",
    "ws_passenger = workbook.add_worksheet('Weekly Passenger Count')\n",
    "fig1, ax1 = plt.subplots(figsize=(12,6))\n",
    "sliced_flights_df.groupby(['weekofyear','terminal_name']).agg({'pax_quantity': 'sum'}).unstack().droplevel(level=0, axis=1).plot(\n",
    "    kind='line', title='Weekly Passenger Count Over Time', ax=ax1\n",
    ")\n",
    "ax1.set_ylim(bottom=0)\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "imgdata1 = io.BytesIO()\n",
    "fig1.savefig(imgdata1, format='png')\n",
    "imgdata1.seek(0)\n",
    "ws_passenger.insert_image('B2', 'weekly_passenger.png', {'image_data': imgdata1})\n",
    "plt.close(fig1)\n",
    "\n",
    "# 2. Average Occupancy per Terminal per Week\n",
    "ws_occupancy = workbook.add_worksheet('Terminal Occupancy')\n",
    "fig2 = plt.figure(figsize=(14, 6))\n",
    "for terminal in master_df['terminal'].unique():\n",
    "    subset = master_df[master_df['terminal'] == terminal]\n",
    "    avg_by_week = subset.groupby('weekofyear')['occupancy'].mean()\n",
    "    plt.plot(avg_by_week.index, avg_by_week.values, label=terminal)\n",
    "plt.xlabel('Week of Year')\n",
    "plt.ylabel('Average Occupancy')\n",
    "plt.title('Average Occupancy per Terminal per Week - 2023')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.gca().set_ylim(bottom=0)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "plt.close(fig2)\n",
    "imgdata2 = io.BytesIO()\n",
    "fig2.savefig(imgdata2, format='png')\n",
    "imgdata2.seek(0)\n",
    "ws_occupancy.insert_image('B2', 'terminal_occupancy.png', {'image_data': imgdata2})\n",
    "\n",
    "# 3. Occupancy Over Time - Terminal/Carpark\n",
    "ws_carpark = workbook.add_worksheet('Carpark Occupancy')\n",
    "fig3_list = []\n",
    "for terminal in master_df['terminal'].unique():\n",
    "    fig3 = plt.figure(figsize=(12, 6))\n",
    "    for carpark in master_df[master_df['terminal'] == terminal]['carpark_name'].unique():\n",
    "        carpark_data = master_df[(master_df['terminal'] == terminal) & (master_df['carpark_name'] == carpark)].groupby(new_second_ledger_column_time_name)['occupancy'].mean()\n",
    "        plt.plot(carpark_data.index, carpark_data.values, label=f\"{carpark}\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Occupancy')\n",
    "    plt.title(f'Occupancy Over Time - Terminal {terminal}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "    imgdata3 = io.BytesIO()\n",
    "    fig3.savefig(imgdata3, format='png')\n",
    "    imgdata3.seek(0)\n",
    "    ws_carpark.insert_image(f'B{2 + 20 * list(master_df[\"terminal\"].unique()).index(terminal)}', f'carpark_{terminal}.png', {'image_data': imgdata3})\n",
    "    plt.close(fig3)\n",
    "\n",
    "# 4. Feature Importances\n",
    "ws_importance = workbook.add_worksheet('Feature Importances')\n",
    "fig4 = plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances_df['Feature'], importances_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Random Forest - Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "imgdata4 = io.BytesIO()\n",
    "fig4.savefig(imgdata4, format='png')\n",
    "imgdata4.seek(0)\n",
    "ws_importance.insert_image('B2', 'feature_importances.png', {'image_data': imgdata4})\n",
    "plt.close(fig4)\n",
    "\n",
    "# 5. Model Metrics Comparison\n",
    "ws_metrics = workbook.add_worksheet('Model Metrics')\n",
    "fig5, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "metrics_df_plot = metrics_df.copy()\n",
    "metrics_df_plot.loc[metrics_df_plot['Model'] == 'Linear Regression', 'Model'] = '(Baseline) Linear Regression'\n",
    "bar_colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "metrics_to_plot = ['R2 Score', 'RMSE', 'MAE', 'MSE']\n",
    "for ax, metric in zip(axes.flatten(), metrics_to_plot):\n",
    "    metrics_df_plot.plot(\n",
    "        x='Model', y=metric, kind='barh', ax=ax, legend=False, color=bar_colors[:len(metrics_df_plot)]\n",
    "    )\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel('Model')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    for x in ax.get_xticks():\n",
    "        ax.axvline(x=x, color='lightgray', linestyle=':', linewidth=0.7, zorder=0)\n",
    "plt.tight_layout()\n",
    "imgdata5 = io.BytesIO()\n",
    "fig5.savefig(imgdata5, format='png')\n",
    "imgdata5.seek(0)\n",
    "ws_metrics.insert_image('B2', 'model_metrics.png', {'image_data': imgdata5})\n",
    "plt.close(fig5)\n",
    "\n",
    "# xgb_result_df\n",
    "\n",
    "workbook.close()\n",
    "output.seek(0)\n",
    "\n",
    "with open(path_to_report, 'wb') as f:\n",
    "    f.write(output.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66830f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
